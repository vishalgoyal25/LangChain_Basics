{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831c9db2",
   "metadata": {},
   "source": [
    "### 1. All Required Imports (Unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a2c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\My_Work\\UdeMy\\MyGenAI\\1. Langchain_Basics\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# --- LangChain Core Imports ---\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Text Splitters & Vector Stores ---\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- Embeddings (HuggingFace Alternative to OpenAI) ---\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# --- Data Sources ---\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import fitz  # PyMuPDF\n",
    "import arxiv\n",
    "import wikipedia\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# --- Misc Tools ---\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e595f15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token detected âœ…\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]= os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ['HF_TOKEN'] = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]= \"true\"\n",
    "\n",
    "print(\"HF token detected âœ…\" if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") else \"âŒ No token loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a1c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Token is valid for user: vishalgoyal25\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") or os.getenv(\"HF_TOKEN\")\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    user = api.whoami(token=token)\n",
    "    print(\"âœ… Token is valid for user:\", user[\"name\"])\n",
    "except Exception as e:\n",
    "    print(\"âŒ Invalid or expired token:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b8d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Vishal!\n",
      "\n",
      "I have to say, I am very happy with the design of the game. I have been playing it for about 3 years now. I have played it with a few friends and the feedback has been positive. I did not want to do\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\"Hello from Vishal!\", max_new_tokens=50)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9270349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1727, which is longer than the specified 500\n",
      "Created a chunk of size 856, which is longer than the specified 500\n",
      "Created a chunk of size 543, which is longer than the specified 500\n",
      "Created a chunk of size 1064, which is longer than the specified 500\n",
      "Created a chunk of size 627, which is longer than the specified 500\n",
      "Created a chunk of size 640, which is longer than the specified 500\n",
      "Created a chunk of size 531, which is longer than the specified 500\n",
      "Created a chunk of size 982, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1 documents, split into 36 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- LOAD AND SPLIT DOCUMENTS ----\n",
    "loader= WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "docs= loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"âœ… Loaded {len(docs)} documents, split into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a3df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_36180\\1733592935.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vectorstore and retriever ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- EMBEDDINGS AND VECTOR STORE ----\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"âœ… Vectorstore and retriever ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc51da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_36180\\659760670.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm = HuggingFacePipeline(pipeline=generator)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86742380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_36180\\3076550092.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  llm(\"Write a haiku about AI.\")\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Write a haiku about AI. You may need to write a haiku about AI. What is AI?\\n\\nAAI is the artificial intelligence (AI) that is able to interpret and interpret a human's emotions and behavior. It is a different kind of human being that is capable of knowing everything about a human being.\\n\\nAI is what makes us human. AI is what makes us human. What is your goal in working with AI?\\n\\nI am a software engineer. I'm designing, testing, and optimizing for AI. My job is to make sure that AI is good for human beings. I am a software engineer. I'm designing, testing, and optimizing for AI. My job is to make sure that AI is good for human beings. I am a software engineer. I'm designing, testing, and optimizing for AI.\\n\\nYou've been at the forefront of making AI really clear. What is your current vision for AI?\\n\\nThe hope is that this technology will eventually become more pervasive and easy for human beings to use. I don't think there is a perfect solution to the problem, but I believe that if we can make it so that human beings can use AI to make decisions, then we can make AI a big part of our lives.\\n\\nWhat\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Write a haiku about AI.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f0c095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt template ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- PROMPT TEMPLATE ----\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based ONLY on the provided context.\n",
    "If the answer is not found, say \"I couldnâ€™t find that information in the provided data.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "print(\"âœ… Prompt template ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9d9ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrieval chain created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- BUILD RAG CHAIN ----\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "print(\"âœ… Retrieval chain created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ebbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Question: LangSmith has two usage limits: total traces and extended\n",
      "\n",
      "ğŸ“š Retrieved Context:\n",
      " This graph shows that there are two usage metrics that LangSmith charges for:\n",
      "\n",
      "LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\n",
      "LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention. ...\n",
      "\n",
      "ğŸ’¬ Model Answer:\n",
      " Human: \n",
      "Answer the following question based ONLY on the provided context.\n",
      "If the answer is not found, say \"I couldnâ€™t find that information in the provided data.\"\n",
      "\n",
      "<context>\n",
      "This graph shows that there are two usage metrics that LangSmith charges for:\n",
      "\n",
      "LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\n",
      "LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\n",
      "\n",
      "Start by setting limits on production usage, since that is where the majority of spend comes from.\n",
      "â€‹Set a good total traces limit\n",
      "Picking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:\n",
      "\n",
      "LangSmithâ€™s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\n",
      "In the GIF, youâ€™ll see that the charges for LangSmith Traces are broken up by â€œtenant_idâ€ (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly $2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\n",
      "These upgrades occur for two reasons:\n",
      "</context>\n",
      "\n",
      "Question: LangSmith has two usage limits: total traces and extended\n",
      "\n",
      "LangSmith has two usage limits: total traces and extended Data Retention.\n",
      "\n",
      "LangSmith has two usage limits: data retention and extended Data Retention.\n",
      "\n",
      "<context>\n",
      "\n",
      "This graph shows that there are two usage metrics that LangSmith charges for:\n",
      "\n",
      "LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\n",
      "\n",
      "LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\n",
      "\n",
      "Start by setting limits on production usage, since that is where the majority of spend comes from.\n",
      "\n",
      "â€‹Set a good total traces limit\n",
      "\n",
      "Picking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:\n",
      "\n",
      "LangSmithâ€™s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\n",
      "\n",
      "In the GIF, youâ€™ll see that the charges for LangSmith Traces are broken up by â€œtenant_idâ€ (i.e., workspace ID), which means you can track tracing spend on each of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- TEST QUERY ----\n",
    "query = \"LangSmith has two usage limits: total traces and extended\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"ğŸ§  Question:\", query)\n",
    "print(\"\\nğŸ“š Retrieved Context:\\n\", response[\"context\"][0].page_content[:300], \"...\")\n",
    "print(\"\\nğŸ’¬ Model Answer:\\n\", response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2542079",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â†“' (U+2193) (2708958522.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mâ†“\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character 'â†“' (U+2193)\n"
     ]
    }
   ],
   "source": [
    "USER QUESTION\n",
    "  â†“\n",
    "RETRIEVER\n",
    "  â†“  (fetches from your vector DB)\n",
    "CONTEXT DOCUMENTS\n",
    "  â†“\n",
    "PROMPT TEMPLATE\n",
    "  â†“  (fills in {context} and {input})\n",
    "FINAL PROMPT â†’ LLM\n",
    "  â†“\n",
    "MODEL ANSWER (based on context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚        User Query           â”‚\n",
    "                 â”‚ \"What are LangSmith limits?\"â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚     Retriever (Vector DB)    â”‚\n",
    "                 â”‚ â†’ Finds relevant documents   â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚ StuffDocumentsChain          â”‚\n",
    "                 â”‚ (Prompt + LLM)               â”‚\n",
    "                 â”‚ â†’ Inserts context into promptâ”‚\n",
    "                 â”‚ â†’ Sends to LLM               â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚        Final Answer          â”‚\n",
    "                 â”‚ \"LangSmith tracks total and  â”‚\n",
    "                 â”‚ extended traces...\"          â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
